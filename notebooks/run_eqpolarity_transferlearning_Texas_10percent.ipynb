{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e38761d-a74d-4be8-a02a-50e1e2b55ee4",
   "metadata": {},
   "source": [
    "First download Texas testing data from \n",
    "https://mega.nz/file/chxx1Z5Y#zXNRKT5aeNy7AGREKEUIq71TREK8hcUyXA1ZOkQ9DlM\n",
    "to the local directory: ../data/\n",
    "\n",
    "Make sure eqpolarity is installed following the instructions on the main website\n",
    "https://github.com/chenyk1990/eqpolarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af177c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import data\n",
    "import numpy as np\n",
    "datall = np.load('../data/TexasData/datall_Texas.npy')\n",
    "polall = np.load('../data/TexasData/polall_Texas.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4ddace3-b2a0-4c65-a6ae-1bb44a94c879",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-09 18:50:41.253468: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input (InputLayer)          [(None, 600, 1)]             0         []                            \n",
      "                                                                                                  \n",
      " cct_tokenizer1 (CCTTokeniz  (None, 150, 200)             160800    ['input[0][0]']               \n",
      " er1)                                                                                             \n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, 150, 200)             400       ['cct_tokenizer1[0][0]']      \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, 150, 200)             642600    ['layer_normalization[0][0]', \n",
      " iHeadAttention)                                                     'layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " stochastic_depth (Stochast  (None, 150, 200)             0         ['multi_head_attention[0][0]']\n",
      " icDepth)                                                                                         \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 150, 200)             0         ['stochastic_depth[0][0]',    \n",
      "                                                                     'cct_tokenizer1[0][0]']      \n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, 150, 200)             400       ['add[0][0]']                 \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 150, 200)             40200     ['layer_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 150, 200)             0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 150, 200)             40200     ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 150, 200)             0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " stochastic_depth_1 (Stocha  (None, 150, 200)             0         ['dropout_1[0][0]']           \n",
      " sticDepth)                                                                                       \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 150, 200)             0         ['stochastic_depth_1[0][0]',  \n",
      "                                                                     'add[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (None, 150, 200)             400       ['add_1[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (None, 150, 200)             642600    ['layer_normalization_2[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " stochastic_depth_2 (Stocha  (None, 150, 200)             0         ['multi_head_attention_1[0][0]\n",
      " sticDepth)                                                         ']                            \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 150, 200)             0         ['stochastic_depth_2[0][0]',  \n",
      "                                                                     'add_1[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_3 (Lay  (None, 150, 200)             400       ['add_2[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 150, 200)             40200     ['layer_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 150, 200)             0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 150, 200)             40200     ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 150, 200)             0         ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " stochastic_depth_3 (Stocha  (None, 150, 200)             0         ['dropout_3[0][0]']           \n",
      " sticDepth)                                                                                       \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 150, 200)             0         ['stochastic_depth_3[0][0]',  \n",
      "                                                                     'add_2[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_4 (Lay  (None, 150, 200)             400       ['add_3[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (Mu  (None, 150, 200)             642600    ['layer_normalization_4[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " stochastic_depth_4 (Stocha  (None, 150, 200)             0         ['multi_head_attention_2[0][0]\n",
      " sticDepth)                                                         ']                            \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 150, 200)             0         ['stochastic_depth_4[0][0]',  \n",
      "                                                                     'add_3[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_5 (Lay  (None, 150, 200)             400       ['add_4[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 150, 200)             40200     ['layer_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 150, 200)             0         ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 150, 200)             40200     ['dropout_4[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 150, 200)             0         ['dense_5[0][0]']             \n",
      "                                                                                                  \n",
      " stochastic_depth_5 (Stocha  (None, 150, 200)             0         ['dropout_5[0][0]']           \n",
      " sticDepth)                                                                                       \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 150, 200)             0         ['stochastic_depth_5[0][0]',  \n",
      "                                                                     'add_4[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_6 (Lay  (None, 150, 200)             400       ['add_5[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (Mu  (None, 150, 200)             642600    ['layer_normalization_6[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " stochastic_depth_6 (Stocha  (None, 150, 200)             0         ['multi_head_attention_3[0][0]\n",
      " sticDepth)                                                         ']                            \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, 150, 200)             0         ['stochastic_depth_6[0][0]',  \n",
      "                                                                     'add_5[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_7 (Lay  (None, 150, 200)             400       ['add_6[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 150, 200)             40200     ['layer_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 150, 200)             0         ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 150, 200)             40200     ['dropout_6[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 150, 200)             0         ['dense_7[0][0]']             \n",
      "                                                                                                  \n",
      " stochastic_depth_7 (Stocha  (None, 150, 200)             0         ['dropout_7[0][0]']           \n",
      " sticDepth)                                                                                       \n",
      "                                                                                                  \n",
      " add_7 (Add)                 (None, 150, 200)             0         ['stochastic_depth_7[0][0]',  \n",
      "                                                                     'add_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_8 (Lay  (None, 150, 200)             400       ['add_7[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 30000)                0         ['layer_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 30000)                0         ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 1)                    30001     ['dropout_8[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3086401 (11.77 MB)\n",
      "Trainable params: 3086401 (11.77 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Load EQpolarity model\n",
    "from eqpolarity.utils import construct_model\n",
    "input_shape = (600,1)\n",
    "model=construct_model(input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b44e5b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 402s 17s/step\n"
     ]
    }
   ],
   "source": [
    "## Load pre-trained model for prediction\n",
    "model.load_weights('../models/best_weigths_Binary_SCSN_Best.h5')\n",
    "out = model.predict(datall,batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ddc39dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9459094865100087,\n",
       " 0.9459094865100087,\n",
       " 0.9459094865100087,\n",
       " 0.9459094865100087)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Applying threshold\n",
    "#outtest = np.argmax(out,axis=-1)\n",
    "thre = 0.5\n",
    "outtest = out\n",
    "outtest[outtest<thre]=0\n",
    "outtest[outtest>=thre]=1\n",
    "labtest = polall\n",
    "\n",
    "## Calculating accuracy, precision, recall, and F1-score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy_score(labtest,outtest),precision_score(labtest,outtest, average='micro'),recall_score(labtest,outtest, average='micro'),f1_score(labtest,outtest, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d953096b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9459094865100087,\n",
       " array([0.97869277, 0.89579665]),\n",
       " array([0.93488276, 0.96491644]),\n",
       " array([0.95628627, 0.92907275]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(labtest,outtest),precision_score(labtest,outtest, average=None),recall_score(labtest,outtest, average=None),f1_score(labtest,outtest, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9c3c81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13596   947]\n",
      " [  296  8141]]\n"
     ]
    }
   ],
   "source": [
    "#Generate the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cf_matrix = confusion_matrix(labtest, outtest)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ac2bf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weightname='best_weigths_Binary_Texas_Transfer10.weights.h5'\n",
    "import datetime\n",
    "today=datetime.date.today()\n",
    "weightname='best_weigths_Binary_Texas_Transfer10_%s.weights.h5'%str(today) #make the weight different by date\n",
    "\n",
    "\n",
    "## training setup\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=weightname,\n",
    "                             monitor='val_acc',\n",
    "                             mode = 'max',\n",
    "                             verbose=1,\n",
    "                             save_weights_only=True,\n",
    "                             save_best_only=True)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=0.1,\n",
    "                                   cooldown=0,\n",
    "                                   patience=50,\n",
    "                                   min_lr=0.5e-6,\n",
    "                                   monitor='val_acc',\n",
    "                                   mode = 'max',\n",
    "                                  verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32299df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0925 - acc: 0.9705  \n",
      "Epoch 1: val_acc improved from -inf to 0.98696, saving model to best_weigths_Binary_Texas_Transfer10_2024-08-09.weights.h5\n",
      "17/17 [==============================] - 1276s 79s/step - loss: 0.0925 - acc: 0.9705 - val_loss: 0.0273 - val_acc: 0.9870 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0532 - acc: 0.9860\n",
      "Epoch 2: val_acc improved from 0.98696 to 0.99130, saving model to best_weigths_Binary_Texas_Transfer10_2024-08-09.weights.h5\n",
      "17/17 [==============================] - 106s 6s/step - loss: 0.0532 - acc: 0.9860 - val_loss: 0.0228 - val_acc: 0.9913 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0432 - acc: 0.9879\n",
      "Epoch 3: val_acc did not improve from 0.99130\n",
      "17/17 [==============================] - 105s 6s/step - loss: 0.0432 - acc: 0.9879 - val_loss: 0.0293 - val_acc: 0.9826 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0360 - acc: 0.9889\n",
      "Epoch 4: val_acc did not improve from 0.99130\n",
      "17/17 [==============================] - 108s 6s/step - loss: 0.0360 - acc: 0.9889 - val_loss: 0.0279 - val_acc: 0.9870 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0346 - acc: 0.9903\n",
      "Epoch 5: val_acc did not improve from 0.99130\n",
      "17/17 [==============================] - 109s 6s/step - loss: 0.0346 - acc: 0.9903 - val_loss: 0.0259 - val_acc: 0.9870 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0278 - acc: 0.9913\n",
      "Epoch 6: val_acc did not improve from 0.99130\n",
      "17/17 [==============================] - 109s 6s/step - loss: 0.0278 - acc: 0.9913 - val_loss: 0.0272 - val_acc: 0.9870 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0260 - acc: 0.9942\n",
      "Epoch 7: val_acc did not improve from 0.99130\n",
      "17/17 [==============================] - 117s 7s/step - loss: 0.0260 - acc: 0.9942 - val_loss: 0.0289 - val_acc: 0.9826 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0197 - acc: 0.9947\n",
      "Epoch 8: val_acc did not improve from 0.99130\n",
      "17/17 [==============================] - 158s 9s/step - loss: 0.0197 - acc: 0.9947 - val_loss: 0.0303 - val_acc: 0.9870 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0161 - acc: 0.9966\n",
      "Epoch 9: val_acc did not improve from 0.99130\n",
      "17/17 [==============================] - 108s 6s/step - loss: 0.0161 - acc: 0.9966 - val_loss: 0.0301 - val_acc: 0.9870 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0128 - acc: 0.9976 \n",
      "Epoch 10: val_acc did not improve from 0.99130\n",
      "17/17 [==============================] - 728s 45s/step - loss: 0.0128 - acc: 0.9976 - val_loss: 0.0250 - val_acc: 0.9870 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0117 - acc: 0.9971\n",
      "Epoch 11: val_acc did not improve from 0.99130\n",
      "17/17 [==============================] - 122s 7s/step - loss: 0.0117 - acc: 0.9971 - val_loss: 0.0255 - val_acc: 0.9870 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0063 - acc: 0.9990\n",
      "Epoch 12: val_acc did not improve from 0.99130\n",
      "17/17 [==============================] - 117s 7s/step - loss: 0.0063 - acc: 0.9990 - val_loss: 0.0205 - val_acc: 0.9913 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0066 - acc: 0.9985  \n",
      "Epoch 13: val_acc did not improve from 0.99130\n",
      "17/17 [==============================] - 1882s 117s/step - loss: 0.0066 - acc: 0.9985 - val_loss: 0.0231 - val_acc: 0.9913 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0045 - acc: 0.9985\n",
      "Epoch 14: val_acc did not improve from 0.99130\n",
      "17/17 [==============================] - 112s 7s/step - loss: 0.0045 - acc: 0.9985 - val_loss: 0.0260 - val_acc: 0.9913 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0072 - acc: 0.9976\n",
      "Epoch 15: val_acc did not improve from 0.99130\n",
      "17/17 [==============================] - 129s 8s/step - loss: 0.0072 - acc: 0.9976 - val_loss: 0.0319 - val_acc: 0.9913 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0119 - acc: 0.9966\n",
      "Epoch 16: val_acc did not improve from 0.99130\n",
      "17/17 [==============================] - 114s 7s/step - loss: 0.0119 - acc: 0.9966 - val_loss: 0.0226 - val_acc: 0.9913 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0081 - acc: 0.9966\n",
      "Epoch 17: val_acc did not improve from 0.99130\n",
      "17/17 [==============================] - 108s 6s/step - loss: 0.0081 - acc: 0.9966 - val_loss: 0.0206 - val_acc: 0.9913 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0036 - acc: 0.9995\n",
      "Epoch 18: val_acc did not improve from 0.99130\n",
      "17/17 [==============================] - 108s 6s/step - loss: 0.0036 - acc: 0.9995 - val_loss: 0.0228 - val_acc: 0.9913 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 19: val_acc did not improve from 0.99130\n",
      "17/17 [==============================] - 109s 6s/step - loss: 0.0021 - acc: 0.9995 - val_loss: 0.0260 - val_acc: 0.9913 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0024 - acc: 0.9995\n",
      "Epoch 20: val_acc did not improve from 0.99130\n",
      "17/17 [==============================] - 117s 7s/step - loss: 0.0024 - acc: 0.9995 - val_loss: 0.0282 - val_acc: 0.9913 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 21: val_acc did not improve from 0.99130\n",
      "17/17 [==============================] - 113s 7s/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0312 - val_acc: 0.9913 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 22: val_acc did not improve from 0.99130\n",
      "17/17 [==============================] - 112s 7s/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0298 - val_acc: 0.9913 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.7171e-04 - acc: 1.0000\n",
      "Epoch 23: val_acc did not improve from 0.99130\n",
      "17/17 [==============================] - 116s 7s/step - loss: 8.7171e-04 - acc: 1.0000 - val_loss: 0.0272 - val_acc: 0.9913 - lr: 0.0010\n",
      "Epoch 24/50\n",
      " 5/17 [=======>......................] - ETA: 1:18 - loss: 2.6050e-04 - acc: 1.0000"
     ]
    }
   ],
   "source": [
    "ind = np.random.permutation(len(datall))\n",
    "a = int(10*len(ind)/100)\n",
    "ind = ind[0:a]\n",
    "x = datall[ind]\n",
    "y = polall[ind]\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=['binary_crossentropy'], metrics=['acc'])\n",
    "model.load_weights('../models/best_weigths_Binary_SCSN_Best.h5')\n",
    "\n",
    "model.fit(x, y, batch_size=128, epochs=50, verbose =1, validation_split=0.1, shuffle=True, callbacks=[checkpoint,lr_reducer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031d0d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(weightname)\n",
    "out = model.predict(datall,batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddeb3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outtest = np.argmax(out,axis=-1)\n",
    "thre = 0.5\n",
    "outtest = out\n",
    "outtest[outtest<thre]=0\n",
    "outtest[outtest>=thre]=1\n",
    "labtest = polall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02cce3e-7bac-4f93-8729-7fac0058097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting different types of accuracy\n",
    "accuracy_score(labtest,outtest),precision_score(labtest,outtest, average='micro'),recall_score(labtest,outtest, average='micro'),f1_score(labtest,outtest, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645086a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(labtest,outtest),precision_score(labtest,outtest, average=None),recall_score(labtest,outtest, average=None),f1_score(labtest,outtest, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201a9cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(labtest,outtest),precision_score(labtest,outtest, average='macro'),recall_score(labtest,outtest, average='macro'),f1_score(labtest,outtest, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81840271",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(labtest, outtest)\n",
    "print(cf_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c12bf97-1042-465c-890f-cb002ac53cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot confusion matrix\n",
    "plot_confusionmatrix(cf=cf_matrix,categories=['Up','Down'],figname='Conf_Matrix_after_transferlearning.png',ifshow=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346dfb88-b4c7-4320-b598-5f3c81e1c0e1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
