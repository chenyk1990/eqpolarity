{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af177c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "datall = np.load('./TexasData/datall_Texas.npy')\n",
    "polall = np.load('./TexasData/polall_Texas.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e372ce03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/envs/tf-gpu-39/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf1\n",
    "tf1.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e4cafee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "from scipy import signal\n",
    "from scipy.signal import butter, lfilter\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Conv1D, MaxPooling1D, UpSampling1D, Flatten, Dropout, Reshape \n",
    "from tensorflow.keras.layers import Bidirectional, concatenate, BatchNormalization, ZeroPadding1D, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import savefig\n",
    "from tensorflow.python.keras.layers import Layer, InputSpec\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Activation, Add, Bidirectional, Conv1D, Dense, Dropout, Embedding, Flatten, Reshape, multiply\n",
    "from keras.layers import concatenate, GRU, Input, LSTM, MaxPooling1D\n",
    "from keras.layers import GlobalAveragePooling1D,  GlobalMaxPooling1D, SpatialDropout1D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "429e1139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        #x = layers.Dense(units, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "065e6343",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rate = 0.2\n",
    "stochastic_depth_rate = 0.1\n",
    "\n",
    "w1 = 100\n",
    "\n",
    "positional_emb = False\n",
    "conv_layers = 2\n",
    "num_classes = 1\n",
    "input_shape = (600,1)\n",
    "image_size = 600  # We'll resize input images to this size\n",
    "projection_dim = int(2*w1)\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4026302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class CCTTokenizer1(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size=4,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        pooling_kernel_size=3,\n",
    "        pooling_stride=(2,2,2,2,2,2,2,2),\n",
    "        num_conv_layers=conv_layers,\n",
    "        num_output_channels=[int(projection_dim), int(projection_dim), int(projection_dim), int(projection_dim), int(projection_dim), int(projection_dim), int(projection_dim), int(projection_dim)],\n",
    "        positional_emb=positional_emb,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(CCTTokenizer1, self).__init__(**kwargs)\n",
    "\n",
    "        # This is our tokenizer.\n",
    "        self.conv_model = tf.keras.Sequential()\n",
    "        for i in range(num_conv_layers):\n",
    "            self.conv_model.add(\n",
    "                layers.Conv1D(\n",
    "                    num_output_channels[i],\n",
    "                    kernel_size,\n",
    "                    stride,\n",
    "                    padding=\"same\",\n",
    "                    use_bias=False,\n",
    "                    activation=\"relu\",\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                )\n",
    "            )\n",
    "            #self.conv_model.add(layers.ZeroPadding2D(padding))\n",
    "            self.conv_model.add(\n",
    "                layers.MaxPool1D(pooling_kernel_size, (pooling_stride[i]), \"same\")\n",
    "            )\n",
    "\n",
    "        self.positional_emb = positional_emb\n",
    "\n",
    "    def call(self, images):\n",
    "        outputs = self.conv_model(images)\n",
    "        # After passing the images through our mini-network the spatial dimensions\n",
    "        # are flattened to form sequences.\n",
    "        reshaped = tf.reshape(\n",
    "            outputs,\n",
    "            (-1, tf.shape(outputs)[1], tf.shape(outputs)[-1]),\n",
    "        )\n",
    "        return reshaped\n",
    "\n",
    "    def positional_embedding(self, image_size):\n",
    "        # Positional embeddings are optional in CCT. Here, we calculate\n",
    "        # the number of sequences and initialize an `Embedding` layer to\n",
    "        # compute the positional embeddings later.\n",
    "        if self.positional_emb:\n",
    "            dummy_inputs = tf.ones((1, image_size, 1))\n",
    "            dummy_outputs = self.call(dummy_inputs)\n",
    "            sequence_length = dummy_outputs.shape[1]\n",
    "            projection_dim = dummy_outputs.shape[-1]\n",
    "\n",
    "            print(dummy_outputs,sequence_length,projection_dim)\n",
    "            embed_layer = layers.Embedding(\n",
    "                input_dim=sequence_length, output_dim=projection_dim\n",
    "            )\n",
    "            return embed_layer, sequence_length\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdabf35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referred from: github.com:rwightman/pytorch-image-models.\n",
    "class StochasticDepth(layers.Layer):\n",
    "    def __init__(self, drop_prop, **kwargs):\n",
    "        super(StochasticDepth, self).__init__(**kwargs)\n",
    "        self.drop_prob = drop_prop\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if training:\n",
    "            keep_prob = 1 - self.drop_prob\n",
    "            shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n",
    "            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
    "            random_tensor = tf.floor(random_tensor)\n",
    "            return (x / keep_prob) * random_tensor\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0db68754",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def create_vit_classifier(inputs):\n",
    "def create_cct_model1(inputs):\n",
    "\n",
    "\n",
    "    # Augment data.\n",
    "    #augmented = data_augmentation(inputs)\n",
    "\n",
    "    # Encode patches.\n",
    "    cct_tokenizer = CCTTokenizer1()\n",
    "    encoded_patches = cct_tokenizer(inputs)\n",
    "\n",
    "    # Apply positional embedding.\n",
    "    if positional_emb:\n",
    "        pos_embed, seq_length = cct_tokenizer.positional_embedding(image_size)\n",
    "        positions = tf.range(start=0, limit=seq_length, delta=1)\n",
    "        position_embeddings = pos_embed(positions)\n",
    "        encoded_patches += position_embeddings\n",
    "\n",
    "    # Calculate Stochastic Depth probabilities.\n",
    "    dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for i in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.2\n",
    "        )(x1, x1)\n",
    "\n",
    "        #print(encoded_patches)\n",
    "        # Skip connection 1.\n",
    "        attention_output = StochasticDepth(dpr[i])(attention_output)\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n",
    "        #x3 = x2\n",
    "        \n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.2)\n",
    "\n",
    "        # Skip connection 2.\n",
    "        #print(x3)\n",
    "        x3 = StochasticDepth(dpr[i])(x3)\n",
    "        #print(x3)\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "     \n",
    "    # Apply sequence pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "    \n",
    "    ''' \n",
    "    attention_weights = tf.nn.softmax(layers.Dense(1)(representation), axis=1)\n",
    "    weighted_representation = tf.matmul(\n",
    "        attention_weights, representation, transpose_a=True\n",
    "    )\n",
    "    weighted_representation = tf.squeeze(weighted_representation, -2)\n",
    "    '''\n",
    "    return representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa3e1220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 600, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " cct_tokenizer1 (CCTTokenizer1)  (None, 150, 200)    160800      ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 150, 200)    400         ['cct_tokenizer1[0][0]']         \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 150, 200)    642600      ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " stochastic_depth (StochasticDe  (None, 150, 200)    0           ['multi_head_attention[0][0]']   \n",
      " pth)                                                                                             \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 150, 200)     0           ['stochastic_depth[0][0]',       \n",
      "                                                                  'cct_tokenizer1[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 150, 200)    400         ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 150, 200)     40200       ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 150, 200)     0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 150, 200)     40200       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 150, 200)     0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " stochastic_depth_1 (Stochastic  (None, 150, 200)    0           ['dropout_2[0][0]']              \n",
      " Depth)                                                                                           \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 150, 200)     0           ['stochastic_depth_1[0][0]',     \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 150, 200)    400         ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 150, 200)    642600      ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " stochastic_depth_2 (Stochastic  (None, 150, 200)    0           ['multi_head_attention_1[0][0]'] \n",
      " Depth)                                                                                           \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 150, 200)     0           ['stochastic_depth_2[0][0]',     \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 150, 200)    400         ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 150, 200)     40200       ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 150, 200)     0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 150, 200)     40200       ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 150, 200)     0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " stochastic_depth_3 (Stochastic  (None, 150, 200)    0           ['dropout_5[0][0]']              \n",
      " Depth)                                                                                           \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 150, 200)     0           ['stochastic_depth_3[0][0]',     \n",
      "                                                                  'add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 150, 200)    400         ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 150, 200)    642600      ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " stochastic_depth_4 (Stochastic  (None, 150, 200)    0           ['multi_head_attention_2[0][0]'] \n",
      " Depth)                                                                                           \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 150, 200)     0           ['stochastic_depth_4[0][0]',     \n",
      "                                                                  'add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 150, 200)    400         ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 150, 200)     40200       ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 150, 200)     0           ['dense_4[0][0]']                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 150, 200)     40200       ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 150, 200)     0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " stochastic_depth_5 (Stochastic  (None, 150, 200)    0           ['dropout_8[0][0]']              \n",
      " Depth)                                                                                           \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 150, 200)     0           ['stochastic_depth_5[0][0]',     \n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 150, 200)    400         ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 150, 200)    642600      ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " stochastic_depth_6 (Stochastic  (None, 150, 200)    0           ['multi_head_attention_3[0][0]'] \n",
      " Depth)                                                                                           \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 150, 200)     0           ['stochastic_depth_6[0][0]',     \n",
      "                                                                  'add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 150, 200)    400         ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 150, 200)     40200       ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 150, 200)     0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 150, 200)     40200       ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 150, 200)     0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " stochastic_depth_7 (Stochastic  (None, 150, 200)    0           ['dropout_11[0][0]']             \n",
      " Depth)                                                                                           \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 150, 200)     0           ['stochastic_depth_7[0][0]',     \n",
      "                                                                  'add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 150, 200)    400         ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 30000)        0           ['layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 1)            30001       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,086,401\n",
      "Trainable params: 3,086,401\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = layers.Input(shape=input_shape,name='input')\n",
    "\n",
    "featuresP = create_cct_model1(inputs)\n",
    "#featuresP = layers.Dropout(0.2)(featuresP)\n",
    "featuresP = layers.Flatten()(featuresP)\n",
    "logitp = layers.Dense(1, activation='sigmoid')(featuresP)\n",
    "\n",
    "\n",
    "#logitp  = Conv2D(1,  3, strides =(1), padding='same',activation='sigmoid', kernel_initializer='he_normal',name='picker_P')(featuresP)\n",
    "\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[logitp])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1417175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 600, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " cct_tokenizer1_1 (CCTTokenizer  (None, 150, 200)    160800      ['input[0][0]']                  \n",
      " 1)                                                                                               \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 150, 200)    400         ['cct_tokenizer1_1[0][0]']       \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (MultiH  (None, 150, 200)    642600      ['layer_normalization_9[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " stochastic_depth_8 (Stochastic  (None, 150, 200)    0           ['multi_head_attention_4[0][0]'] \n",
      " Depth)                                                                                           \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 150, 200)     0           ['stochastic_depth_8[0][0]',     \n",
      "                                                                  'cct_tokenizer1_1[0][0]']       \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 150, 200)    400         ['add_8[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 150, 200)     40200       ['layer_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 150, 200)     0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 150, 200)     40200       ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 150, 200)     0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " stochastic_depth_9 (Stochastic  (None, 150, 200)    0           ['dropout_14[0][0]']             \n",
      " Depth)                                                                                           \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 150, 200)     0           ['stochastic_depth_9[0][0]',     \n",
      "                                                                  'add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 150, 200)    400         ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (MultiH  (None, 150, 200)    642600      ['layer_normalization_11[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " stochastic_depth_10 (Stochasti  (None, 150, 200)    0           ['multi_head_attention_5[0][0]'] \n",
      " cDepth)                                                                                          \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 150, 200)     0           ['stochastic_depth_10[0][0]',    \n",
      "                                                                  'add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 150, 200)    400         ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 150, 200)     40200       ['layer_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 150, 200)     0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 150, 200)     40200       ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 150, 200)     0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " stochastic_depth_11 (Stochasti  (None, 150, 200)    0           ['dropout_17[0][0]']             \n",
      " cDepth)                                                                                          \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 150, 200)     0           ['stochastic_depth_11[0][0]',    \n",
      "                                                                  'add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 150, 200)    400         ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_6 (MultiH  (None, 150, 200)    642600      ['layer_normalization_13[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " stochastic_depth_12 (Stochasti  (None, 150, 200)    0           ['multi_head_attention_6[0][0]'] \n",
      " cDepth)                                                                                          \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 150, 200)     0           ['stochastic_depth_12[0][0]',    \n",
      "                                                                  'add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 150, 200)    400         ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 150, 200)     40200       ['layer_normalization_14[0][0]'] \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dropout_19 (Dropout)           (None, 150, 200)     0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 150, 200)     40200       ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 150, 200)     0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " stochastic_depth_13 (Stochasti  (None, 150, 200)    0           ['dropout_20[0][0]']             \n",
      " cDepth)                                                                                          \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 150, 200)     0           ['stochastic_depth_13[0][0]',    \n",
      "                                                                  'add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 150, 200)    400         ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_7 (MultiH  (None, 150, 200)    642600      ['layer_normalization_15[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " stochastic_depth_14 (Stochasti  (None, 150, 200)    0           ['multi_head_attention_7[0][0]'] \n",
      " cDepth)                                                                                          \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 150, 200)     0           ['stochastic_depth_14[0][0]',    \n",
      "                                                                  'add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_16 (LayerN  (None, 150, 200)    400         ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 150, 200)     40200       ['layer_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 150, 200)     0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 150, 200)     40200       ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 150, 200)     0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " stochastic_depth_15 (Stochasti  (None, 150, 200)    0           ['dropout_23[0][0]']             \n",
      " cDepth)                                                                                          \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 150, 200)     0           ['stochastic_depth_15[0][0]',    \n",
      "                                                                  'add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_17 (LayerN  (None, 150, 200)    400         ['add_15[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 30000)        0           ['layer_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 30000)        0           ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 1)            30001       ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,086,401\n",
      "Trainable params: 3,086,401\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = layers.Input(shape=input_shape,name='input')\n",
    "\n",
    "featuresP = create_cct_model1(inputs)\n",
    "featuresP = layers.Flatten()(featuresP)\n",
    "featuresP = layers.Dropout(0.2)(featuresP)\n",
    "logitp = layers.Dense(1, activation='sigmoid')(featuresP)\n",
    "\n",
    "\n",
    "#logitp  = Conv2D(1,  3, strides =(1), padding='same',activation='sigmoid', kernel_initializer='he_normal',name='picker_P')(featuresP)\n",
    "\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[logitp])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b44e5b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-30 01:57:45.466878: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-30 01:57:46.046360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9214 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:02:00.0, compute capability: 7.5\n",
      "2022-10-30 01:57:46.122432: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('best_weigths_Binary_CSCN_Best.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce106d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/tf-gpu-39/lib/python3.9/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2022-10-30 01:57:47.642029: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8201\n",
      "2022-10-30 01:57:48.283896: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Running ptxas --version returned 32512\n",
      "2022-10-30 01:57:48.355355: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: ptxas exited with non-zero error code 32512, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "out = model.predict(datall,batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ddc39dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.94590948651000872,\n",
       " 0.94590948651000872,\n",
       " 0.94590948651000872,\n",
       " 0.94590948651000872)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "#outtest = np.argmax(out,axis=-1)\n",
    "thre = 0.5\n",
    "outtest = out\n",
    "outtest[outtest<thre]=0\n",
    "outtest[outtest>=thre]=1\n",
    "labtest = polall\n",
    "\n",
    "accuracy_score(labtest,outtest),precision_score(labtest,outtest, average='micro'),recall_score(labtest,outtest, average='micro'),f1_score(labtest,outtest, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d953096b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.94590948651000872,\n",
       " array([ 0.97869277,  0.89579665]),\n",
       " array([ 0.93488276,  0.96491644]),\n",
       " array([ 0.95628627,  0.92907275]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(labtest,outtest),precision_score(labtest,outtest, average=None),recall_score(labtest,outtest, average=None),f1_score(labtest,outtest, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9c3c81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13596   947]\n",
      " [  296  8141]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Generate the confusion matrix\n",
    "cf_matrix = confusion_matrix(labtest, outtest)\n",
    "\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7a85d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 16}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "cf = cf_matrix\n",
    "categories=['Up','Down']\n",
    "group_percentages = []\n",
    "counts = []\n",
    "for i in range(len(cf)):\n",
    "    for j in range(len(cf)):\n",
    "        group_percentages.append(cf[j, i]/np.sum(cf[:, i]))\n",
    "        counts.append(cf[j, i])\n",
    "\n",
    "percentages_matrix = np.reshape(group_percentages, (2, 2))\n",
    "group_percentages = ['{0:.2%}'.format(value) for value in group_percentages]\n",
    "\n",
    "labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_percentages, counts)]\n",
    "labels = np.asarray(labels).reshape(2, 2, order = 'F')\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "sn.set(font_scale=2) # for label size\n",
    "sn.heatmap(percentages_matrix, annot = labels, fmt = '', xticklabels=categories, yticklabels = categories, cbar = False)\n",
    "#fig.savefig('Conf_Matrix',bbox_inches='tight',transparent=True, dpi =100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ac2bf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath='best_weigths_Binary_Texas_Transfer10.h5',\n",
    "                             monitor='val_acc',\n",
    "                             mode = 'max',\n",
    "                             verbose=1,\n",
    "                             save_weights_only=True,\n",
    "                             save_best_only=True)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=0.1,\n",
    "                                   cooldown=0,\n",
    "                                   patience=50,\n",
    "                                   min_lr=0.5e-6,\n",
    "                                   monitor='val_acc',\n",
    "                                   mode = 'max',\n",
    "                                  verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b32299df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2068 samples, validate on 230 samples\n",
      "Epoch 1/50\n",
      "2068/2068 [==============================] - ETA: 0s - loss: 0.0928 - acc: 0.9671"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/tf-gpu-39/lib/python3.9/site-packages/keras/engine/training_v1.py:2045: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_acc improved from -inf to 0.96957, saving model to best_weigths_Binary_Texas_Transfer10.h5\n",
      "2068/2068 [==============================] - 5s 2ms/sample - loss: 0.0928 - acc: 0.9671 - val_loss: 0.0449 - val_acc: 0.9696 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9858\n",
      "Epoch 2: val_acc did not improve from 0.96957\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0520 - acc: 0.9860 - val_loss: 0.0527 - val_acc: 0.9696 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9907\n",
      "Epoch 3: val_acc improved from 0.96957 to 0.97826, saving model to best_weigths_Binary_Texas_Transfer10.h5\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0445 - acc: 0.9903 - val_loss: 0.0538 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9927\n",
      "Epoch 4: val_acc did not improve from 0.97826\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0386 - acc: 0.9927 - val_loss: 0.0559 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9941\n",
      "Epoch 5: val_acc did not improve from 0.97826\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0300 - acc: 0.9937 - val_loss: 0.0603 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9937\n",
      "Epoch 6: val_acc did not improve from 0.97826\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0295 - acc: 0.9937 - val_loss: 0.0535 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9956\n",
      "Epoch 7: val_acc did not improve from 0.97826\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0220 - acc: 0.9956 - val_loss: 0.0643 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9937\n",
      "Epoch 8: val_acc improved from 0.97826 to 0.98261, saving model to best_weigths_Binary_Texas_Transfer10.h5\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0212 - acc: 0.9937 - val_loss: 0.0605 - val_acc: 0.9826 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9966\n",
      "Epoch 9: val_acc did not improve from 0.98261\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0151 - acc: 0.9961 - val_loss: 0.0636 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9985\n",
      "Epoch 10: val_acc improved from 0.98261 to 0.98696, saving model to best_weigths_Binary_Texas_Transfer10.h5\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0115 - acc: 0.9985 - val_loss: 0.0686 - val_acc: 0.9870 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9976\n",
      "Epoch 11: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0079 - acc: 0.9976 - val_loss: 0.0709 - val_acc: 0.9870 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9990\n",
      "Epoch 12: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0065 - acc: 0.9990 - val_loss: 0.0820 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9976\n",
      "Epoch 13: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0085 - acc: 0.9976 - val_loss: 0.0573 - val_acc: 0.9826 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9985\n",
      "Epoch 14: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0074 - acc: 0.9985 - val_loss: 0.0607 - val_acc: 0.9826 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9980\n",
      "Epoch 15: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0087 - acc: 0.9981 - val_loss: 0.0737 - val_acc: 0.9826 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9971\n",
      "Epoch 16: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0056 - acc: 0.9971 - val_loss: 0.0578 - val_acc: 0.9826 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9995\n",
      "Epoch 17: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0037 - acc: 0.9995 - val_loss: 0.0789 - val_acc: 0.9826 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9985\n",
      "Epoch 18: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0042 - acc: 0.9985 - val_loss: 0.0736 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9990\n",
      "Epoch 19: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0032 - acc: 0.9990 - val_loss: 0.0884 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9985\n",
      "Epoch 20: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0045 - acc: 0.9985 - val_loss: 0.0784 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 21: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 3s 1ms/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0725 - val_acc: 0.9826 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 22: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0769 - val_acc: 0.9826 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 6.8520e-04 - acc: 1.0000\n",
      "Epoch 23: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 6.8492e-04 - acc: 1.0000 - val_loss: 0.0727 - val_acc: 0.9826 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 5.6151e-04 - acc: 1.0000\n",
      "Epoch 24: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 5.7685e-04 - acc: 1.0000 - val_loss: 0.0721 - val_acc: 0.9870 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 3.9986e-04 - acc: 1.0000\n",
      "Epoch 25: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 3.9646e-04 - acc: 1.0000 - val_loss: 0.0821 - val_acc: 0.9870 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 1.0000    \n",
      "Epoch 26: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0843 - val_acc: 0.9826 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 4.3250e-04 - acc: 1.0000\n",
      "Epoch 27: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 4.2885e-04 - acc: 1.0000 - val_loss: 0.0817 - val_acc: 0.9826 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 5.0464e-04 - acc: 1.0000\n",
      "Epoch 28: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 5.2327e-04 - acc: 1.0000 - val_loss: 0.0840 - val_acc: 0.9870 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 3.3756e-04 - acc: 1.0000\n",
      "Epoch 29: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 3.3582e-04 - acc: 1.0000 - val_loss: 0.0889 - val_acc: 0.9870 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 30: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0020 - acc: 0.9995 - val_loss: 0.0707 - val_acc: 0.9826 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 31: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0711 - val_acc: 0.9870 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 8.4120e-04 - acc: 0.9995\n",
      "Epoch 32: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 8.3401e-04 - acc: 0.9995 - val_loss: 0.0946 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9985\n",
      "Epoch 33: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0038 - acc: 0.9981 - val_loss: 0.1242 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9966\n",
      "Epoch 34: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0123 - acc: 0.9966 - val_loss: 0.1309 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.2169 - acc: 0.9517\n",
      "Epoch 35: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.2166 - acc: 0.9512 - val_loss: 0.1991 - val_acc: 0.9609 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9697\n",
      "Epoch 36: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0955 - acc: 0.9700 - val_loss: 0.0399 - val_acc: 0.9826 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0687 - acc: 0.9800\n",
      "Epoch 37: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0684 - acc: 0.9797 - val_loss: 0.0368 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9854\n",
      "Epoch 38: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0478 - acc: 0.9855 - val_loss: 0.0403 - val_acc: 0.9826 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9883\n",
      "Epoch 39: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0369 - acc: 0.9884 - val_loss: 0.0417 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9932\n",
      "Epoch 40: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0260 - acc: 0.9932 - val_loss: 0.0437 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9912\n",
      "Epoch 41: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0262 - acc: 0.9913 - val_loss: 0.0415 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9946\n",
      "Epoch 42: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0249 - acc: 0.9947 - val_loss: 0.0491 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9941\n",
      "Epoch 43: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0168 - acc: 0.9942 - val_loss: 0.0520 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9937\n",
      "Epoch 44: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0200 - acc: 0.9937 - val_loss: 0.0596 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 45/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9956\n",
      "Epoch 45: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0154 - acc: 0.9956 - val_loss: 0.0577 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 46/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9966\n",
      "Epoch 46: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0186 - acc: 0.9966 - val_loss: 0.0581 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 47/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9951\n",
      "Epoch 47: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0141 - acc: 0.9952 - val_loss: 0.0558 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 48/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9961\n",
      "Epoch 48: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0120 - acc: 0.9961 - val_loss: 0.0529 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 49/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9961\n",
      "Epoch 49: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0137 - acc: 0.9961 - val_loss: 0.0611 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 50/50\n",
      "2048/2068 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9966\n",
      "Epoch 50: val_acc did not improve from 0.98696\n",
      "2068/2068 [==============================] - 2s 1ms/sample - loss: 0.0114 - acc: 0.9966 - val_loss: 0.0559 - val_acc: 0.9739 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f06df899ee0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "ind = np.random.permutation(len(datall))\n",
    "a = int(10*len(ind)/100)\n",
    "ind = ind[0:a]\n",
    "x = datall[ind]\n",
    "y = polall[ind]\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=['binary_crossentropy'], metrics=['acc'])\n",
    "model.load_weights('best_weigths_Binary_CSCN_Best.h5')\n",
    "\n",
    "model.fit(x, y, batch_size=128, epochs=50, verbose =1, validation_split=0.1, shuffle=True, callbacks=[checkpoint,lr_reducer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "031d0d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/tf-gpu-39/lib/python3.9/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('best_weigths_Binary_Texas_Transfer10.h5')\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "out = model.predict(datall,batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ddeb3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.98572671888598784,\n",
       " 0.98572671888598784,\n",
       " 0.98572671888598784,\n",
       " 0.98572671888598784)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "#outtest = np.argmax(out,axis=-1)\n",
    "thre = 0.5\n",
    "outtest = out\n",
    "outtest[outtest<thre]=0\n",
    "outtest[outtest>=thre]=1\n",
    "labtest = polall\n",
    "\n",
    "accuracy_score(labtest,outtest),precision_score(labtest,outtest, average='micro'),recall_score(labtest,outtest, average='micro'),f1_score(labtest,outtest, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "645086a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.98572671888598784,\n",
       " array([ 0.98912669,  0.97987928]),\n",
       " array([ 0.98831053,  0.98127296]),\n",
       " array([ 0.98871844,  0.98057562]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(labtest,outtest),precision_score(labtest,outtest, average=None),recall_score(labtest,outtest, average=None),f1_score(labtest,outtest, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "201a9cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.98572671888598784,\n",
       " 0.98450298515336732,\n",
       " 0.98479174586265539,\n",
       " 0.98464703368634043)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(labtest,outtest),precision_score(labtest,outtest, average='macro'),recall_score(labtest,outtest, average='macro'),f1_score(labtest,outtest, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81840271",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-39",
   "language": "python",
   "name": "tf-gpu-39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
