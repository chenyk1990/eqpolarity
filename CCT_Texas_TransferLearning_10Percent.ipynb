{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af177c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "datall = np.load('./TexasData/datall_Texas.npy')\n",
    "polall = np.load('./TexasData/polall_Texas.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "049ee2c2-3309-4072-9410-11645f10afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install h5py\n",
    "# !pip install scipy\n",
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e372ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow.compat.v1 as tf1\n",
    "# tf1.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e4cafee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 20:17:31.024290: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "from scipy import signal\n",
    "from scipy.signal import butter, lfilter\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Conv1D, MaxPooling1D, UpSampling1D, Flatten, Dropout, Reshape \n",
    "from tensorflow.keras.layers import Bidirectional, concatenate, BatchNormalization, ZeroPadding1D, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import savefig\n",
    "from tensorflow.python.keras.layers import Layer, InputSpec\n",
    "# import tensorflow as tf\n",
    "# tf.compat.v1.reset_default_graph()\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Activation, Add, Bidirectional, Conv1D, Dense, Dropout, Embedding, Flatten, Reshape, multiply\n",
    "from keras.layers import concatenate, GRU, Input, LSTM, MaxPooling1D\n",
    "from keras.layers import GlobalAveragePooling1D,  GlobalMaxPooling1D, SpatialDropout1D\n",
    "from keras.models import Model\n",
    "# from keras.preprocessing import text, sequence\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "429e1139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        #x = layers.Dense(units, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "065e6343",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rate = 0.2\n",
    "stochastic_depth_rate = 0.1\n",
    "\n",
    "w1 = 100\n",
    "\n",
    "positional_emb = False\n",
    "conv_layers = 2\n",
    "num_classes = 1\n",
    "input_shape = (600,1)\n",
    "image_size = 600  # We'll resize input images to this size\n",
    "projection_dim = int(2*w1)\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4026302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class CCTTokenizer1(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size=4,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        pooling_kernel_size=3,\n",
    "        pooling_stride=(2,2,2,2,2,2,2,2),\n",
    "        num_conv_layers=conv_layers,\n",
    "        num_output_channels=[int(projection_dim), int(projection_dim), int(projection_dim), int(projection_dim), int(projection_dim), int(projection_dim), int(projection_dim), int(projection_dim)],\n",
    "        positional_emb=positional_emb,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(CCTTokenizer1, self).__init__(**kwargs)\n",
    "\n",
    "        # This is our tokenizer.\n",
    "        self.conv_model = tf.keras.Sequential()\n",
    "        for i in range(num_conv_layers):\n",
    "            self.conv_model.add(\n",
    "                layers.Conv1D(\n",
    "                    num_output_channels[i],\n",
    "                    kernel_size,\n",
    "                    stride,\n",
    "                    padding=\"same\",\n",
    "                    use_bias=False,\n",
    "                    activation=\"relu\",\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                )\n",
    "            )\n",
    "            #self.conv_model.add(layers.ZeroPadding2D(padding))\n",
    "            self.conv_model.add(\n",
    "                layers.MaxPool1D(pooling_kernel_size, (pooling_stride[i]), \"same\")\n",
    "            )\n",
    "\n",
    "        self.positional_emb = positional_emb\n",
    "\n",
    "    def call(self, images):\n",
    "        outputs = self.conv_model(images)\n",
    "        # After passing the images through our mini-network the spatial dimensions\n",
    "        # are flattened to form sequences.\n",
    "        reshaped = tf.reshape(\n",
    "            outputs,\n",
    "            (-1, tf.shape(outputs)[1], tf.shape(outputs)[-1]),\n",
    "        )\n",
    "        return reshaped\n",
    "\n",
    "    def positional_embedding(self, image_size):\n",
    "        # Positional embeddings are optional in CCT. Here, we calculate\n",
    "        # the number of sequences and initialize an `Embedding` layer to\n",
    "        # compute the positional embeddings later.\n",
    "        if self.positional_emb:\n",
    "            dummy_inputs = tf.ones((1, image_size, 1))\n",
    "            dummy_outputs = self.call(dummy_inputs)\n",
    "            sequence_length = dummy_outputs.shape[1]\n",
    "            projection_dim = dummy_outputs.shape[-1]\n",
    "\n",
    "            print(dummy_outputs,sequence_length,projection_dim)\n",
    "            embed_layer = layers.Embedding(\n",
    "                input_dim=sequence_length, output_dim=projection_dim\n",
    "            )\n",
    "            return embed_layer, sequence_length\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdabf35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referred from: github.com:rwightman/pytorch-image-models.\n",
    "class StochasticDepth(layers.Layer):\n",
    "    def __init__(self, drop_prop, **kwargs):\n",
    "        super(StochasticDepth, self).__init__(**kwargs)\n",
    "        self.drop_prob = drop_prop\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if training:\n",
    "            keep_prob = 1 - self.drop_prob\n",
    "            shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n",
    "            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
    "            random_tensor = tf.floor(random_tensor)\n",
    "            return (x / keep_prob) * random_tensor\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0db68754",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def create_vit_classifier(inputs):\n",
    "def create_cct_model1(inputs):\n",
    "\n",
    "\n",
    "    # Augment data.\n",
    "    #augmented = data_augmentation(inputs)\n",
    "\n",
    "    # Encode patches.\n",
    "    cct_tokenizer = CCTTokenizer1()\n",
    "    encoded_patches = cct_tokenizer(inputs)\n",
    "\n",
    "    # Apply positional embedding.\n",
    "    if positional_emb:\n",
    "        pos_embed, seq_length = cct_tokenizer.positional_embedding(image_size)\n",
    "        positions = tf.range(start=0, limit=seq_length, delta=1)\n",
    "        position_embeddings = pos_embed(positions)\n",
    "        encoded_patches += position_embeddings\n",
    "\n",
    "    # Calculate Stochastic Depth probabilities.\n",
    "    dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for i in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.2\n",
    "        )(x1, x1)\n",
    "\n",
    "        #print(encoded_patches)\n",
    "        # Skip connection 1.\n",
    "        attention_output = StochasticDepth(dpr[i])(attention_output)\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n",
    "        #x3 = x2\n",
    "        \n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.2)\n",
    "\n",
    "        # Skip connection 2.\n",
    "        #print(x3)\n",
    "        x3 = StochasticDepth(dpr[i])(x3)\n",
    "        #print(x3)\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "     \n",
    "    # Apply sequence pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "    \n",
    "    ''' \n",
    "    attention_weights = tf.nn.softmax(layers.Dense(1)(representation), axis=1)\n",
    "    weighted_representation = tf.matmul(\n",
    "        attention_weights, representation, transpose_a=True\n",
    "    )\n",
    "    weighted_representation = tf.squeeze(weighted_representation, -2)\n",
    "    '''\n",
    "    return representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1417175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input (InputLayer)          [(None, 600, 1)]             0         []                            \n",
      "                                                                                                  \n",
      " cct_tokenizer1_1 (CCTToken  (None, 150, 200)             160800    ['input[0][0]']               \n",
      " izer1)                                                                                           \n",
      "                                                                                                  \n",
      " layer_normalization_9 (Lay  (None, 150, 200)             400       ['cct_tokenizer1_1[0][0]']    \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (Mu  (None, 150, 200)             642600    ['layer_normalization_9[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_9[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " stochastic_depth_8 (Stocha  (None, 150, 200)             0         ['multi_head_attention_4[0][0]\n",
      " sticDepth)                                                         ']                            \n",
      "                                                                                                  \n",
      " add_8 (Add)                 (None, 150, 200)             0         ['stochastic_depth_8[0][0]',  \n",
      "                                                                     'cct_tokenizer1_1[0][0]']    \n",
      "                                                                                                  \n",
      " layer_normalization_10 (La  (None, 150, 200)             400       ['add_8[0][0]']               \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 150, 200)             40200     ['layer_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 150, 200)             0         ['dense_9[0][0]']             \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 150, 200)             40200     ['dropout_8[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)         (None, 150, 200)             0         ['dense_10[0][0]']            \n",
      "                                                                                                  \n",
      " stochastic_depth_9 (Stocha  (None, 150, 200)             0         ['dropout_9[0][0]']           \n",
      " sticDepth)                                                                                       \n",
      "                                                                                                  \n",
      " add_9 (Add)                 (None, 150, 200)             0         ['stochastic_depth_9[0][0]',  \n",
      "                                                                     'add_8[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_11 (La  (None, 150, 200)             400       ['add_9[0][0]']               \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (Mu  (None, 150, 200)             642600    ['layer_normalization_11[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " stochastic_depth_10 (Stoch  (None, 150, 200)             0         ['multi_head_attention_5[0][0]\n",
      " asticDepth)                                                        ']                            \n",
      "                                                                                                  \n",
      " add_10 (Add)                (None, 150, 200)             0         ['stochastic_depth_10[0][0]', \n",
      "                                                                     'add_9[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_12 (La  (None, 150, 200)             400       ['add_10[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 150, 200)             40200     ['layer_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)        (None, 150, 200)             0         ['dense_11[0][0]']            \n",
      "                                                                                                  \n",
      " dense_12 (Dense)            (None, 150, 200)             40200     ['dropout_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)        (None, 150, 200)             0         ['dense_12[0][0]']            \n",
      "                                                                                                  \n",
      " stochastic_depth_11 (Stoch  (None, 150, 200)             0         ['dropout_11[0][0]']          \n",
      " asticDepth)                                                                                      \n",
      "                                                                                                  \n",
      " add_11 (Add)                (None, 150, 200)             0         ['stochastic_depth_11[0][0]', \n",
      "                                                                     'add_10[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_13 (La  (None, 150, 200)             400       ['add_11[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_6 (Mu  (None, 150, 200)             642600    ['layer_normalization_13[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " stochastic_depth_12 (Stoch  (None, 150, 200)             0         ['multi_head_attention_6[0][0]\n",
      " asticDepth)                                                        ']                            \n",
      "                                                                                                  \n",
      " add_12 (Add)                (None, 150, 200)             0         ['stochastic_depth_12[0][0]', \n",
      "                                                                     'add_11[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_14 (La  (None, 150, 200)             400       ['add_12[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_13 (Dense)            (None, 150, 200)             40200     ['layer_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)        (None, 150, 200)             0         ['dense_13[0][0]']            \n",
      "                                                                                                  \n",
      " dense_14 (Dense)            (None, 150, 200)             40200     ['dropout_12[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)        (None, 150, 200)             0         ['dense_14[0][0]']            \n",
      "                                                                                                  \n",
      " stochastic_depth_13 (Stoch  (None, 150, 200)             0         ['dropout_13[0][0]']          \n",
      " asticDepth)                                                                                      \n",
      "                                                                                                  \n",
      " add_13 (Add)                (None, 150, 200)             0         ['stochastic_depth_13[0][0]', \n",
      "                                                                     'add_12[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_15 (La  (None, 150, 200)             400       ['add_13[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_7 (Mu  (None, 150, 200)             642600    ['layer_normalization_15[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " stochastic_depth_14 (Stoch  (None, 150, 200)             0         ['multi_head_attention_7[0][0]\n",
      " asticDepth)                                                        ']                            \n",
      "                                                                                                  \n",
      " add_14 (Add)                (None, 150, 200)             0         ['stochastic_depth_14[0][0]', \n",
      "                                                                     'add_13[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_16 (La  (None, 150, 200)             400       ['add_14[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_15 (Dense)            (None, 150, 200)             40200     ['layer_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)        (None, 150, 200)             0         ['dense_15[0][0]']            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)            (None, 150, 200)             40200     ['dropout_14[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)        (None, 150, 200)             0         ['dense_16[0][0]']            \n",
      "                                                                                                  \n",
      " stochastic_depth_15 (Stoch  (None, 150, 200)             0         ['dropout_15[0][0]']          \n",
      " asticDepth)                                                                                      \n",
      "                                                                                                  \n",
      " add_15 (Add)                (None, 150, 200)             0         ['stochastic_depth_15[0][0]', \n",
      "                                                                     'add_14[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_17 (La  (None, 150, 200)             400       ['add_15[0][0]']              \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 30000)                0         ['layer_normalization_17[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)        (None, 30000)                0         ['flatten_1[0][0]']           \n",
      "                                                                                                  \n",
      " dense_17 (Dense)            (None, 1)                    30001     ['dropout_16[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3086401 (11.77 MB)\n",
      "Trainable params: 3086401 (11.77 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = layers.Input(shape=input_shape,name='input')\n",
    "\n",
    "featuresP = create_cct_model1(inputs)\n",
    "featuresP = layers.Flatten()(featuresP)\n",
    "featuresP = layers.Dropout(0.2)(featuresP)\n",
    "logitp = layers.Dense(1, activation='sigmoid')(featuresP)\n",
    "\n",
    "\n",
    "#logitp  = Conv2D(1,  3, strides =(1), padding='same',activation='sigmoid', kernel_initializer='he_normal',name='picker_P')(featuresP)\n",
    "\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[logitp])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b44e5b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_weigths_Binary_CSCN_Best.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce106d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 331s 14s/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "out = model.predict(datall,batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ddc39dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9459094865100087,\n",
       " 0.9459094865100087,\n",
       " 0.9459094865100087,\n",
       " 0.9459094865100087)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "#outtest = np.argmax(out,axis=-1)\n",
    "thre = 0.5\n",
    "outtest = out\n",
    "outtest[outtest<thre]=0\n",
    "outtest[outtest>=thre]=1\n",
    "labtest = polall\n",
    "\n",
    "accuracy_score(labtest,outtest),precision_score(labtest,outtest, average='micro'),recall_score(labtest,outtest, average='micro'),f1_score(labtest,outtest, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d953096b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9459094865100087,\n",
       " array([0.97869277, 0.89579665]),\n",
       " array([0.93488276, 0.96491644]),\n",
       " array([0.95628627, 0.92907275]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(labtest,outtest),precision_score(labtest,outtest, average=None),recall_score(labtest,outtest, average=None),f1_score(labtest,outtest, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9c3c81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13596   947]\n",
      " [  296  8141]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Generate the confusion matrix\n",
    "cf_matrix = confusion_matrix(labtest, outtest)\n",
    "\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7a85d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 16}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "cf = cf_matrix\n",
    "categories=['Up','Down']\n",
    "group_percentages = []\n",
    "counts = []\n",
    "for i in range(len(cf)):\n",
    "    for j in range(len(cf)):\n",
    "        group_percentages.append(cf[j, i]/np.sum(cf[:, i]))\n",
    "        counts.append(cf[j, i])\n",
    "\n",
    "percentages_matrix = np.reshape(group_percentages, (2, 2))\n",
    "group_percentages = ['{0:.2%}'.format(value) for value in group_percentages]\n",
    "\n",
    "labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_percentages, counts)]\n",
    "labels = np.asarray(labels).reshape(2, 2, order = 'F')\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "sn.set(font_scale=2) # for label size\n",
    "sn.heatmap(percentages_matrix, annot = labels, fmt = '', xticklabels=categories, yticklabels = categories, cbar = False)\n",
    "#fig.savefig('Conf_Matrix',bbox_inches='tight',transparent=True, dpi =100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ac2bf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath='best_weigths_Binary_Texas_Transfer10.weights.h5',\n",
    "                             monitor='val_acc',\n",
    "                             mode = 'max',\n",
    "                             verbose=1,\n",
    "                             save_weights_only=True,\n",
    "                             save_best_only=True)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=0.1,\n",
    "                                   cooldown=0,\n",
    "                                   patience=50,\n",
    "                                   min_lr=0.5e-6,\n",
    "                                   monitor='val_acc',\n",
    "                                   mode = 'max',\n",
    "                                  verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b32299df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0736 - acc: 0.9686\n",
      "Epoch 1: val_acc improved from -inf to 0.97826, saving model to best_weigths_Binary_Texas_Transfer10.weights.h5\n",
      "17/17 [==============================] - 122s 7s/step - loss: 0.0736 - acc: 0.9686 - val_loss: 0.1043 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0400 - acc: 0.9860\n",
      "Epoch 2: val_acc improved from 0.97826 to 0.98261, saving model to best_weigths_Binary_Texas_Transfer10.weights.h5\n",
      "17/17 [==============================] - 110s 6s/step - loss: 0.0400 - acc: 0.9860 - val_loss: 0.0971 - val_acc: 0.9826 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0315 - acc: 0.9908\n",
      "Epoch 3: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 113s 7s/step - loss: 0.0315 - acc: 0.9908 - val_loss: 0.1101 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0284 - acc: 0.9918\n",
      "Epoch 4: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 113s 7s/step - loss: 0.0284 - acc: 0.9918 - val_loss: 0.1114 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0236 - acc: 0.9956\n",
      "Epoch 5: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 121s 7s/step - loss: 0.0236 - acc: 0.9956 - val_loss: 0.1062 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0212 - acc: 0.9932\n",
      "Epoch 6: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 119s 7s/step - loss: 0.0212 - acc: 0.9932 - val_loss: 0.1104 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0146 - acc: 0.9966\n",
      "Epoch 7: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 114s 7s/step - loss: 0.0146 - acc: 0.9966 - val_loss: 0.1285 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0102 - acc: 0.9981\n",
      "Epoch 8: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 115s 7s/step - loss: 0.0102 - acc: 0.9981 - val_loss: 0.1228 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0091 - acc: 0.9976\n",
      "Epoch 9: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 117s 7s/step - loss: 0.0091 - acc: 0.9976 - val_loss: 0.1296 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0076 - acc: 0.9976\n",
      "Epoch 10: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 113s 7s/step - loss: 0.0076 - acc: 0.9976 - val_loss: 0.1357 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0051 - acc: 0.9985\n",
      "Epoch 11: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 115s 7s/step - loss: 0.0051 - acc: 0.9985 - val_loss: 0.1521 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0056 - acc: 0.9990\n",
      "Epoch 12: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 114s 7s/step - loss: 0.0056 - acc: 0.9990 - val_loss: 0.1627 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0050 - acc: 0.9985\n",
      "Epoch 13: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 107s 6s/step - loss: 0.0050 - acc: 0.9985 - val_loss: 0.1737 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0037 - acc: 0.9990\n",
      "Epoch 14: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 118s 7s/step - loss: 0.0037 - acc: 0.9990 - val_loss: 0.1855 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0056 - acc: 0.9990\n",
      "Epoch 15: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 116s 7s/step - loss: 0.0056 - acc: 0.9990 - val_loss: 0.1731 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 16: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 110s 6s/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.1921 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 17: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 112s 7s/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.1929 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0032 - acc: 0.9990\n",
      "Epoch 18: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 115s 7s/step - loss: 0.0032 - acc: 0.9990 - val_loss: 0.1839 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0024 - acc: 0.9995\n",
      "Epoch 19: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 111s 7s/step - loss: 0.0024 - acc: 0.9995 - val_loss: 0.1872 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0025 - acc: 0.9995\n",
      "Epoch 20: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 138s 8s/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.1921 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0069 - acc: 0.9985 \n",
      "Epoch 21: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 170s 10s/step - loss: 0.0069 - acc: 0.9985 - val_loss: 0.1870 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0034 - acc: 0.9990\n",
      "Epoch 22: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 160s 9s/step - loss: 0.0034 - acc: 0.9990 - val_loss: 0.1849 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 23: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 171s 10s/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.1852 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.3317e-04 - acc: 1.0000\n",
      "Epoch 24: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 134s 8s/step - loss: 6.3317e-04 - acc: 1.0000 - val_loss: 0.2010 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 5.0284e-04 - acc: 1.0000\n",
      "Epoch 25: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 119s 7s/step - loss: 5.0284e-04 - acc: 1.0000 - val_loss: 0.2147 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 4.3756e-04 - acc: 1.0000\n",
      "Epoch 26: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 113s 7s/step - loss: 4.3756e-04 - acc: 1.0000 - val_loss: 0.2217 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 5.3356e-04 - acc: 1.0000\n",
      "Epoch 27: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 107s 6s/step - loss: 5.3356e-04 - acc: 1.0000 - val_loss: 0.2224 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 3.0486e-04 - acc: 1.0000\n",
      "Epoch 28: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 110s 6s/step - loss: 3.0486e-04 - acc: 1.0000 - val_loss: 0.2289 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 3.2937e-04 - acc: 1.0000\n",
      "Epoch 29: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 110s 6s/step - loss: 3.2937e-04 - acc: 1.0000 - val_loss: 0.2379 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.6590e-04 - acc: 1.0000\n",
      "Epoch 30: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 111s 7s/step - loss: 2.6590e-04 - acc: 1.0000 - val_loss: 0.2444 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.5945e-04 - acc: 1.0000\n",
      "Epoch 31: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 120s 7s/step - loss: 2.5945e-04 - acc: 1.0000 - val_loss: 0.2472 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.9075e-04 - acc: 1.0000\n",
      "Epoch 32: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 125s 7s/step - loss: 1.9075e-04 - acc: 1.0000 - val_loss: 0.2382 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.8207e-04 - acc: 1.0000\n",
      "Epoch 33: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 133s 8s/step - loss: 1.8207e-04 - acc: 1.0000 - val_loss: 0.2484 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.4510e-04 - acc: 1.0000\n",
      "Epoch 34: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 132s 8s/step - loss: 2.4510e-04 - acc: 1.0000 - val_loss: 0.2509 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.0443e-04 - acc: 1.0000\n",
      "Epoch 35: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 128s 8s/step - loss: 2.0443e-04 - acc: 1.0000 - val_loss: 0.2609 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0302e-04 - acc: 1.0000\n",
      "Epoch 36: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 141s 8s/step - loss: 1.0302e-04 - acc: 1.0000 - val_loss: 0.2722 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2279e-04 - acc: 1.0000\n",
      "Epoch 37: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 139s 8s/step - loss: 1.2279e-04 - acc: 1.0000 - val_loss: 0.2744 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.5446e-04 - acc: 1.0000\n",
      "Epoch 38: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 140s 8s/step - loss: 2.5446e-04 - acc: 1.0000 - val_loss: 0.2864 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 3.7122e-04 - acc: 1.0000\n",
      "Epoch 39: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 136s 8s/step - loss: 3.7122e-04 - acc: 1.0000 - val_loss: 0.2640 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.7213e-04 - acc: 1.0000\n",
      "Epoch 40: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 144s 8s/step - loss: 1.7213e-04 - acc: 1.0000 - val_loss: 0.2663 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1095e-04 - acc: 1.0000\n",
      "Epoch 41: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 130s 8s/step - loss: 1.1095e-04 - acc: 1.0000 - val_loss: 0.2722 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.9990e-05 - acc: 1.0000\n",
      "Epoch 42: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 124s 7s/step - loss: 8.9990e-05 - acc: 1.0000 - val_loss: 0.2741 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0417e-04 - acc: 1.0000\n",
      "Epoch 43: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 123s 7s/step - loss: 1.0417e-04 - acc: 1.0000 - val_loss: 0.2847 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.5683e-05 - acc: 1.0000\n",
      "Epoch 44: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 140s 8s/step - loss: 7.5683e-05 - acc: 1.0000 - val_loss: 0.2919 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 5.8577e-05 - acc: 1.0000\n",
      "Epoch 45: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 146s 9s/step - loss: 5.8577e-05 - acc: 1.0000 - val_loss: 0.2973 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 3.8488e-04 - acc: 1.0000\n",
      "Epoch 46: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 127s 7s/step - loss: 3.8488e-04 - acc: 1.0000 - val_loss: 0.2877 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.9638e-04 - acc: 0.9990\n",
      "Epoch 47: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 133s 8s/step - loss: 9.9638e-04 - acc: 0.9990 - val_loss: 0.2678 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0012 - acc: 0.9995  \n",
      "Epoch 48: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 140s 8s/step - loss: 0.0012 - acc: 0.9995 - val_loss: 0.2458 - val_acc: 0.9783 - lr: 0.0010\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.6444e-04 - acc: 0.9995\n",
      "Epoch 49: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 118s 7s/step - loss: 7.6444e-04 - acc: 0.9995 - val_loss: 0.3124 - val_acc: 0.9739 - lr: 0.0010\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0020 - acc: 0.9990\n",
      "Epoch 50: val_acc did not improve from 0.98261\n",
      "17/17 [==============================] - 116s 7s/step - loss: 0.0020 - acc: 0.9990 - val_loss: 0.2519 - val_acc: 0.9739 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x14413ef90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "ind = np.random.permutation(len(datall))\n",
    "a = int(10*len(ind)/100)\n",
    "ind = ind[0:a]\n",
    "x = datall[ind]\n",
    "y = polall[ind]\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=['binary_crossentropy'], metrics=['acc'])\n",
    "model.load_weights('best_weigths_Binary_CSCN_Best.h5')\n",
    "\n",
    "model.fit(x, y, batch_size=128, epochs=50, verbose =1, validation_split=0.1, shuffle=True, callbacks=[checkpoint,lr_reducer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "031d0d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 363s 16s/step\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('best_weigths_Binary_Texas_Transfer10.weights.h5')\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "out = model.predict(datall,batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ddeb3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9857267188859878,\n",
       " 0.9857267188859878,\n",
       " 0.9857267188859878,\n",
       " 0.9857267188859878)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "#outtest = np.argmax(out,axis=-1)\n",
    "thre = 0.5\n",
    "outtest = out\n",
    "outtest[outtest<thre]=0\n",
    "outtest[outtest>=thre]=1\n",
    "labtest = polall\n",
    "\n",
    "accuracy_score(labtest,outtest),precision_score(labtest,outtest, average='micro'),recall_score(labtest,outtest, average='micro'),f1_score(labtest,outtest, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "645086a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9857267188859878,\n",
       " array([0.98912669, 0.97987928]),\n",
       " array([0.98831053, 0.98127296]),\n",
       " array([0.98871844, 0.98057562]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(labtest,outtest),precision_score(labtest,outtest, average=None),recall_score(labtest,outtest, average=None),f1_score(labtest,outtest, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "201a9cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9857267188859878,\n",
       " 0.9845029851533673,\n",
       " 0.9847917458626554,\n",
       " 0.9846470336863404)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(labtest,outtest),precision_score(labtest,outtest, average='macro'),recall_score(labtest,outtest, average='macro'),f1_score(labtest,outtest, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81840271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14373   170]\n",
      " [  158  8279]]\n"
     ]
    }
   ],
   "source": [
    "cf_matrix = confusion_matrix(labtest, outtest)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c12bf97-1042-465c-890f-cb002ac53cba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
